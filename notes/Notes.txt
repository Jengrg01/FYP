1. Model Training
Model training refers to the process of teaching a machine learning model how to make predictions based on input data.

Steps in Model Training
Data Collection: Gather relevant data for training.
Preprocessing: Clean and normalize the data to improve model performance.
Splitting Data: Typically, data is divided into:
Training Set (used to train the model)
Validation Set (used to tune hyperparameters)
Test Set (used to evaluate performance)
Model Selection: Choose an appropriate model architecture (e.g., CNN, RNN, Transformer).
Forward Propagation: Compute predictions using initial model parameters.
Loss Function: Measures the error between predicted and actual values (e.g., MSE, Cross-Entropy).
Backward Propagation: Adjust model parameters using gradients to minimize loss.
Optimization Algorithm: Uses techniques like SGD (Stochastic Gradient Descent) or Adam to update weights.
Evaluation: Assess model performance using metrics like accuracy, precision, recall, F1-score.
2. Fine-Tuning
Fine-tuning is a technique where a pre-trained model is adjusted for a specific task by training it further on a new dataset.

Steps in Fine-Tuning
Select a Pre-trained Model (e.g., ResNet, VGG, BERT).
Freeze Lower Layers: Keep early layers unchanged (they capture general features).
Train Higher Layers: Modify and retrain later layers to adapt to new data.
Adjust Learning Rate: Use a smaller learning rate to avoid destroying pre-trained knowledge.
Regularization: Prevent overfitting using techniques like dropout or weight decay.
Evaluation & Deployment: Test the model and deploy it for real-world tasks.
Fine-tuning is commonly used in transfer learning, where a model trained on a large dataset (like ImageNet) is adapted for smaller datasets.

3. Convolutional Neural Networks (CNNs)
CNNs are a type of neural network used primarily for image processing and computer vision tasks. They automatically detect patterns and features in images.

Key Components of CNNs
Convolutional Layers
Extract features from input images using filters/kernels.
Each filter detects edges, textures, or objects.
Activation Function (ReLU)
Introduces non-linearity to help detect complex patterns.
Pooling Layers
Reduce feature map size and computation.
Max Pooling: Selects the maximum value in a region.
Average Pooling: Takes the average of values in a region.
Fully Connected Layers (FC)
Flatten feature maps and connect to output neurons for classification.
Dropout
Randomly disables neurons during training to prevent overfitting.
Common CNN Architectures
LeNet-5: Early CNN model for digit recognition.
AlexNet: Popularized deep learning for image classification.
VGG16/VGG19: Uses deeper layers for better accuracy.
ResNet: Introduces residual connections to solve vanishing gradient problems.
EfficientNet: Optimized for performance with fewer parameters.