Introduction to LeNet-5
Developed by: Yann LeCun and his team in 1998.
Purpose: Created for handwritten digit recognition, primarily using the MNIST dataset.
Significance: One of the first convolutional neural network (CNN) architectures that showcased the potential of deep learning in image recognition.
Architecture Overview
LeNet-5 comprises seven layers (excluding the input) with trainable parameters.

Input Layer: Accepts 32x32 grayscale images.
Layer 1 - Convolutional Layer (C1)
Filters: 6 filters of size 5x5.
Output: 6 feature maps of 28x28.
Activation Function: Sigmoid.
Layer 2 - Subsampling Layer (S2)
Type: Average pooling.
Filter Size: 2x2 with a stride of 2.
Output: 6 feature maps of 14x14.
Layer 3 - Convolutional Layer (C3)
Filters: 16 filters of size 5x5.
Output: 16 feature maps of 10x10.
Activation Function: Sigmoid.
Layer 4 - Subsampling Layer (S4)
Type: Average pooling.
Filter Size: 2x2 with a stride of 2.
Output: 16 feature maps of 5x5.
Layer 5 - Convolutional Layer (C5)
Filters: 120 filters of size 5x5.
Output: 120 feature maps of 1x1.
Activation Function: Sigmoid.
Layer 6 - Fully Connected Layer (F6)
Neurons: 84.
Activation Function: Sigmoid.
Output Layer
Neurons: 10 (corresponding to digits 0-9).
Activation Function: Softmax, providing a probability distribution over the classes.
Key Features
Convolutional Layers: Learn spatial hierarchies of features from input images.
Subsampling (Pooling) Layers: Reduce computational complexity while enhancing translation invariance.
Fully Connected Layers: Aggregate extracted features for classification.
Training Details
Dataset: MNIST handwritten digits.
Loss Function: Mean Squared Error (MSE).
Optimizer: Stochastic Gradient Descent (SGD).
Impact and Legacy
Pioneering CNN Architecture: Laid the groundwork for modern deep learning models.
Proved Practicality of Neural Networks: Demonstrated that deep learning could effectively handle image recognition tasks.






